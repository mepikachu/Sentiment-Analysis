{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Combine all toxic labels into one\n",
    "df_train['label'] = df_train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].max(axis=1)\n",
    "df_test['label'] = df_test[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].max(axis=1)\n",
    "\n",
    "Y_train = df_train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n",
    "Y_test = df_test[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n",
    "\n",
    "# Tokenize the text data and convert it to sequences\n",
    "max_words = 20000  # Vocabulary size\n",
    "max_len = 100      # Maximum length of each comment\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df_train['comment_text'])\n",
    "X_train = tokenizer.texts_to_sequences(df_train['comment_text'])\n",
    "X_test = tokenizer.texts_to_sequences(df_test['comment_text'])\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "with open('models/tokenizer.pkl', 'wb') as f:\n",
    "    joblib.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pikachu/.local/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3990/3990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 53ms/step - accuracy: 0.7268 - loss: 0.1277 - val_accuracy: 0.9941 - val_loss: 0.0519\n",
      "Epoch 2/5\n",
      "\u001b[1m3990/3990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 55ms/step - accuracy: 0.9216 - loss: 0.0512 - val_accuracy: 0.9941 - val_loss: 0.0497\n",
      "Epoch 3/5\n",
      "\u001b[1m3990/3990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 56ms/step - accuracy: 0.8902 - loss: 0.0447 - val_accuracy: 0.9941 - val_loss: 0.0536\n",
      "Epoch 4/5\n",
      "\u001b[1m3990/3990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 45ms/step - accuracy: 0.8540 - loss: 0.0406 - val_accuracy: 0.9941 - val_loss: 0.0568\n",
      "Epoch 5/5\n",
      "\u001b[1m3990/3990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 42ms/step - accuracy: 0.8343 - loss: 0.0370 - val_accuracy: 0.9941 - val_loss: 0.0580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n",
      "Evaluating model on the test dataset...\n",
      "\u001b[1m4787/4787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 13ms/step\n",
      "Accuracy(test): 0.8058029301924734\n",
      "F1 Score(test): 0.2492022645393721\n",
      "Precision(test): 0.15317452785422797\n",
      "Recall(test): 0.6679542005793903\n"
     ]
    }
   ],
   "source": [
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6, activation='sigmoid'))  # Six output nodes for multi-label classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = model.fit(X_train, Y_train, epochs=5, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save('models/toxic_rnn_classifier.h5')\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating model on the test dataset...\")\n",
    "Y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "f1 = f1_score(Y_test, Y_pred, average='micro')\n",
    "precision = precision_score(Y_test, Y_pred, average='micro')\n",
    "recall = recall_score(Y_test, Y_pred, average='micro')\n",
    "print(f\"Accuracy(test): {accuracy}\")\n",
    "print(f\"F1 Score(test): {f1}\")\n",
    "print(f\"Precision(test): {precision}\")\n",
    "print(f\"Recall(test): {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying example comments...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
      "Comment: I really love this!\n",
      "Labels: [0 0 0 0 0 0]\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Comment: This is the worst thing ever.\n",
      "Labels: [1 0 0 0 0 0]\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Comment: You are an idiot and should be banned!\n",
      "Labels: [1 0 1 0 1 0]\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Comment: The government needs to address this issue.\n",
      "Labels: [0 0 0 0 0 0]\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Comment: You fucking idiot, I am gonna make you suffer.\n",
      "Labels: [1 0 1 0 1 0]\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Comment: that's fucking awesome.\n",
      "Labels: [1 0 1 0 1 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Making predictions using the model\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "model = load_model('models/toxic_rnn_classifier.h5')\n",
    "with open('models/tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = joblib.load(f)\n",
    "\n",
    "def predict(comment):\n",
    "    comment_seq = tokenizer.texts_to_sequences([comment])\n",
    "    comment_pad = pad_sequences(comment_seq, maxlen=max_len)\n",
    "    prediction = (model.predict(comment_pad) > 0.5).astype(int)\n",
    "    return prediction[0]\n",
    "\n",
    "comments = [\n",
    "    \"I really love this!\",\n",
    "    \"This is the worst thing ever.\",\n",
    "    \"You are an idiot and should be banned!\",\n",
    "    \"The government needs to address this issue.\",\n",
    "    \"You fucking idiot, I am gonna make you suffer.\",\n",
    "    \"that's fucking awesome.\"\n",
    "]\n",
    "\n",
    "print(\"Classifying example comments...\")\n",
    "for comment in comments:\n",
    "    labels = predict(comment)\n",
    "    print(f\"Comment: {comment}\\nLabels: {labels}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
